{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will use word embeddings to compute the similarities between documents. \n",
    "\n",
    "The dataset is provided with the homework and is the same dataset that was provided with Homework 2. It contains reviews for six of the review topics used in the paper <i>John Blitzer, Mark Dredze, and Fernando Pereira: Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007).</i> The data has been formatted so that there is one review per line, and the texts have been tokenized and normalized. \n",
    "\n",
    "A line in the file is organized in columns as follows:\n",
    "\n",
    "0: <b> topic</b> category label (books, camera, dvd, health, music, or software) <br>\n",
    "1: <b>sentiment</b> category label (pos or neg) <br>\n",
    "2: document identifier <br>\n",
    "3 and on: the actual review  <br>\n",
    "    \n",
    "    \n",
    "Here is an example of a line:\n",
    "\n",
    "<b>health pos 691.txt</b> <i>smaller size did not fit me or my son . it would be nice if you could use perhaps a hat size to determine right size . standard size worked fine . </i>\n",
    "\n",
    "<b> You will use only the reviews (not the categories) for this assignment. </b>\n",
    "\n",
    "You will need to use word embeddings for this assignment. I have provided code for using Glove Embeddings below. You may download the actual embeddings from https://nlp.stanford.edu/projects/glove/ . Note that this is a 822MB file. There are 4 different dimensional embeddings with this download (50d, 100d, 200d, 300d). The higher dimensional embeddings require more disk space but give better results. <b>I have included the 50d and 100d files with this submission, so you don't have to download the 822MB file. </b> You will only need to use ONE kind of embedding in this assignment, and you will not be penalized for using 50d. \n",
    "\n",
    "Alternatively, you may use other embeddings such as FastText https://fasttext.cc/docs/en/english-vectors.html . \n",
    "\n",
    "IMPORTANT: Your submission must be a Jupyter Notebook (.ipynb) file that is organized into sections as in this notebook file. You should edit this file to insert your answers and submit the resulting file. You can insert cells as needed, however keep your code well-organized and easy to understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the imports below\n",
    "# include any other imports you need here\n",
    "# you may have to install gensim using \"pip install gensim\"\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global random seed\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Corpus - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function named <i>read_corpus</i> that takes as parameters (a) a filename for the dataset and (b) the number of lines (<i>N</i>) to be read from the file starting from the beginning of the file. The function should return the first <i>N</i> reviews as a list. The first review will be at position 0 in the list, the second review at position 1, and so on.\n",
    "\n",
    "(1) Invoke your function with the filename of the dataset provided with this assignment and 2000 as the number of lines. Save the result in a variable named <i>corpus</i>.\n",
    "\n",
    "(2) Print out the folowing: <br>\n",
    " (i) any two reviews <br>\n",
    " (ii) number of reviews (should be 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART I: insert code for the function below\n",
    "def read_corpus(filename, N=-1):\n",
    "    \"\"\"\n",
    "    Given a filename and the number of lines N to read, return a list of the first N lines,\n",
    "    Skip the first 3 tokens as they are metadata and not reviews\n",
    "    \"\"\"\n",
    "    corpus = list()\n",
    "    with open(filename) as f:\n",
    "        i = 0            \n",
    "        for line in f:                \n",
    "            tokens = line.strip().split()\n",
    "            data = tokens[3:]\n",
    "            corpus.append(' '.join(data))\n",
    "            i += 1\n",
    "\n",
    "            #If N is equal to -1, we go to the end of the file. Else we read only the first N reviews\n",
    "            if N > 0 and i >= N:\n",
    "                break\n",
    "        \n",
    "                \n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW #0: \n",
      "i bought this album because i loved the title song . it 's such a great song , how bad can the rest of the album be , right ? well , the rest of the songs are just filler and are n't worth the money i paid for this . it 's either shameless bubblegum or oversentimentalized depressing tripe . kenny chesney is a popular artist and as a result he is in the cookie cutter category of the nashville music scene . he 's gotta pump out the albums so the record company can keep lining their pockets while the suckers out there keep buying this garbage to perpetuate more garbage coming out of that town . i 'll get down off my soapbox now . but country music really needs to get back to it 's roots and stop this pop nonsense . what country music really is and what it is considered to be by mainstream are two different things .\n",
      "REVIEW #2: \n",
      "i have introduced many of my ell , high school students to lois lowery and the depth of her characters . she is a brilliant writer and capable of inspiring fierce passion in her readers as they encounter shocking details of her utopian worlds . i was anxious to read this companion novel and had planned to share it with my class this january . although the series is written for 6th graders and older , this book 's simplicity , in its message , language and writing style will inspire no one . i am sadly disappointed\n",
      "# REVIEWS IN CORPUS: 2000\n"
     ]
    }
   ],
   "source": [
    "# code for \n",
    "#(1) invoking the read_corpus function\n",
    "#(2) generating output for  \n",
    "    #(i) any two reviews \n",
    "    #(ii) number of reviews\n",
    "### fill your code below this line and run this cell to SHOW YOUR OUTPUT\n",
    "\n",
    "path_glove100d = '../data/HW3_data/glove.6B.100d.txt'\n",
    "path_glove50d = '../data/HW3_data/glove.6B.50d.txt'\n",
    "\n",
    "path_reviews = \"../data/HW3_data/all_reviews.txt\"\n",
    "corpus = read_corpus(path_reviews,N=2000)\n",
    "len(corpus)\n",
    "\n",
    "print(\"REVIEW #0: \")\n",
    "print(corpus[0])\n",
    "\n",
    "print(\"REVIEW #2: \")\n",
    "print(corpus[2])\n",
    "\n",
    "\n",
    "print(\"# REVIEWS IN CORPUS:\", len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Embedding Vectors - 10 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Glove embeddings, you can use the code below to load the model and access the embeddings in the model. \n",
    "\n",
    "Alternatively, you can read the embeddings file (it is a text file, so you should inspect it) into word vectors in memory. \n",
    "\n",
    "Note that if you are using other embeddings such as FastText or directly reading the embeddings file into word vectors, you will need to write your own code and replace the code below with your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PART II: load word embeddings.\n",
    "# If you are using the 50d or another dimensional embedding,\n",
    "# you will need to replace the name of the file in the path below\n",
    "glove_file = datapath('/Users/yanivbronshtein/Coding/Rutgers/NLP_Rutgers_Repo/data/HW3_data/glove.6B.100d.txt')\n",
    "word2vec_text_file = get_tmpfile(\"glove.6B.100d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may take a while to load\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_text_file) #Gets all of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the deprecated warning if you get it\n",
    "# wv contains the word vectors/embeddings\n",
    "comp_wv = model.get_vector('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6298e-01,  3.0141e-01,  5.7978e-01,  6.6548e-02,  4.5835e-01,\n",
       "       -1.5329e-01,  4.3258e-01, -8.9215e-01,  5.7747e-01,  3.6375e-01,\n",
       "        5.6524e-01, -5.6281e-01,  3.5659e-01, -3.6096e-01, -9.9662e-02,\n",
       "        5.2753e-01,  3.8839e-01,  9.6185e-01,  1.8841e-01,  3.0741e-01,\n",
       "       -8.7842e-01, -3.2442e-01,  1.1202e+00,  7.5126e-02,  4.2661e-01,\n",
       "       -6.0651e-01, -1.3893e-01,  4.7862e-02, -4.5158e-01,  9.3723e-02,\n",
       "        1.7463e-01,  1.0962e+00, -1.0044e+00,  6.3889e-02,  3.8002e-01,\n",
       "        2.1109e-01, -6.6247e-01, -4.0736e-01,  8.9442e-01, -6.0974e-01,\n",
       "       -1.8577e-01, -1.9913e-01, -6.9226e-01, -3.1806e-01, -7.8565e-01,\n",
       "        2.3831e-01,  1.2992e-01,  8.7721e-02,  4.3205e-01, -2.2662e-01,\n",
       "        3.1549e-01, -3.1748e-01, -2.4632e-03,  1.6615e-01,  4.2358e-01,\n",
       "       -1.8087e+00, -3.6699e-01,  2.3949e-01,  2.5458e+00,  3.6111e-01,\n",
       "        3.9486e-02,  4.8607e-01, -3.6974e-01,  5.7282e-02, -4.9317e-01,\n",
       "        2.2765e-01,  7.9966e-01,  2.1428e-01,  6.9811e-01,  1.1262e+00,\n",
       "       -1.3526e-01,  7.1972e-01, -9.9605e-04, -2.6842e-01, -8.3038e-01,\n",
       "        2.1780e-01,  3.4355e-01,  3.7731e-01, -4.0251e-01,  3.3124e-01,\n",
       "        1.2576e+00, -2.7196e-01, -8.6093e-01,  9.0053e-02, -2.4876e+00,\n",
       "        4.5200e-01,  6.6945e-01, -5.4648e-01, -1.0324e-01, -1.6979e-01,\n",
       "        5.9437e-01,  1.1280e+00,  7.5755e-01, -5.9160e-02,  1.5152e-01,\n",
       "       -2.8388e-01,  4.9452e-01, -9.1703e-01,  9.1289e-01, -3.0927e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will correspond to the size of your embedding\n",
    "len(comp_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Word Similarity - 10 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Print the first 20 values in the embeddings for 2 words of your choice (these should not be <i> computer</i>).\n",
    "\n",
    "(2) Compute and print the cosine similarity between 5 pairs of words that are close and far apart in meaning (E.g. (car, automobile) (water, digital))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code in this cell and below. Insert new cells as need\n",
    "chess_wv = model.get_vector('chess')\n",
    "style_wv = model.get_vector('style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7635e-01,  5.4567e-01,  3.0534e-01,  9.0395e-01, -8.8172e-02,\n",
       "        6.2945e-01,  4.0376e-01, -8.1160e-01, -1.9370e-01, -3.1395e-01,\n",
       "       -1.6067e-02, -6.8291e-01, -1.2400e-02, -2.0827e-01, -1.0267e+00,\n",
       "        1.4386e+00,  5.1816e-01,  2.0026e-01, -8.3672e-04, -2.9563e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chess_wv[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10472  ,  0.48403  , -0.0086339,  0.024064 , -0.31165  ,\n",
       "        0.93899  ,  0.11214  , -0.5615   , -0.81848  ,  1.3578   ,\n",
       "       -0.47941  , -0.37561  , -0.035467 ,  0.26547  , -0.31471  ,\n",
       "        0.69005  , -0.17193  , -0.37695  , -0.20739  , -0.30336  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_wv[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68319416"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('car', 'automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09676659"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('skiing', 'soldering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20361975"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('water', 'digital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62254065"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('ice', 'hockey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732221"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('soccer', 'football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030391458"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('electricity', 'tennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6801137"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"happy\", \"sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV:  Document Embeddings - 30 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function named <i>calc_doc_embedding</i> that takes the following parameters \n",
    "\n",
    "(1) a document (string) <br>\n",
    "(2) embeddings <br>\n",
    "(3) embedding size (dimensions of embedding) <br>\n",
    "(4) OOV_embed (a dictionary) <br>\n",
    "\n",
    "The function should return the embedding of the document using the centroid of the vectors of all the words in the document. The formula is given in Equation 6.23 in Section 6.7 of the Required Reading. \n",
    "\n",
    "Note that when you pass a document to this function, there may be words in the document that are not in the vocabulary of the embeddings such as proper nouns. You can choose to ignore out of vocabulary (OOV) words, however you may not get great results for document similarity. One approach you could try is to assign a random embedding to an OOV word when you see it for the first time and add the word and its generated embedding to the dictionary <i>OOV_embed</i>. You can then check both <i>embeddings</i> and <i>OOV_embed</i> to retrieve the embedding for a word inside the function. \n",
    "\n",
    "It may also help to ignore words in a stopset (you can use stopwords from nltk.corpus for this). You can use the code below for setting up a stopset. Insert a cell below to include and run this code.<br>\n",
    "<i>from nltk.corpus import stopwords<br>\n",
    "import string<br>\n",
    "stopset = stopwords.words('english') + list(string.punctuation)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART IV: insert code for function\n",
    "def calc_doc_embedding(doc, embeddings, embed_size, OOV_embed):\n",
    "    \"\"\"\n",
    "    doc:string\n",
    "    embeddings: a list of vectors where each vector has 100 values\n",
    "    embed_size:usually 100 but could be something else\n",
    "    OOV_embed:dict where \n",
    "        key:word, \n",
    "        value: vector of 100 values that fall under the uniform distribution\n",
    "    \n",
    "    This function calculates the document embeddings\n",
    "    \"\"\"\n",
    "    stopset = stopwords.words('english') + list(string.punctuation)\n",
    "    matched_embeddings = list() #\n",
    "    # Filter stop words from the document\n",
    "    tok_doc = [i for i in doc.strip().lower().split() if i not in stopset] \n",
    "    \n",
    "    unique_tok_doc = list(set(tok_doc)) #Remove duplicate words\n",
    "    \n",
    "    #iterate through every unique document token and create a random entry in OOV_embed if the word\n",
    "    #does not exist in the embeddings or if it does not exist in OOV_embed\n",
    "    for word in unique_tok_doc:\n",
    "        if word in embeddings or word in OOV_embed:\n",
    "            matched_embeddings.append(embeddings.get_vector(word))\n",
    "        else:\n",
    "            OOV_embed[word] = np.random.uniform(-1, 1, embed_size)\n",
    "    \n",
    "    #compute and return the centroid\n",
    "    oov_values = list(OOV_embed.values())\n",
    "    total_embeddings = matched_embeddings + oov_values\n",
    "    mat = np.matrix(total_embeddings)\n",
    "    doc_embed = np.sum(mat, axis=0)/ len(total_embeddings)\n",
    "    return doc_embed\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V - Document Similarity - 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function <i> calc_doc_embeddings </i>, compute and print the cosine similarity for the following pairs of sentences:\n",
    "\n",
    "(1) \"I want to book a ticket\", \"I want to purchase a ticket\"<br>\n",
    "(2) \"I want to book a ticket\", \"I want to reserve a ticket\"<br>\n",
    "(3) \"I want to book a ticket\", \"I want to go golfing\"<br>\n",
    "(4) \"I want to eat a sandwich for lunch\", \"I want to eat a tuna sandwich for lunch\"<br>\n",
    "(5) \"I want to eat a fish sandwich for lunch\", \"I want to eat a tuna sandwich for lunch\"\n",
    "\n",
    "Before you invoke <i> calc_doc_embeddings </i> for the first time, you will need to initialize OOV_embed to an empty dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = [\n",
    "    [\"I want to book a ticket\", \"I want to purchase a ticket\"],\n",
    "    [\"I want to book a ticket\", \"I want to reserve a ticket\"],\n",
    "    [\"I want to book a ticket\", \"I want to go golfing\"],\n",
    "    [\"I want to eat a sandwich for lunch\", \"I want to eat a tuna sandwich for lunch\"],\n",
    "    [\"I want to eat a fish sandwich for lunch\", \"I want to eat a tuna sandwich for lunch\"],  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) ['I want to book a ticket', 'I want to purchase a ticket']\n",
      "COSINE SIMILARITY: 0.88618135\n",
      "*****************************************************************\n",
      "(2) ['I want to book a ticket', 'I want to reserve a ticket']\n",
      "COSINE SIMILARITY: 0.8330232\n",
      "*****************************************************************\n",
      "(3) ['I want to book a ticket', 'I want to go golfing']\n",
      "COSINE SIMILARITY: 0.7701012\n",
      "*****************************************************************\n",
      "(4) ['I want to eat a sandwich for lunch', 'I want to eat a tuna sandwich for lunch']\n",
      "COSINE SIMILARITY: 0.9700427\n",
      "*****************************************************************\n",
      "(5) ['I want to eat a fish sandwich for lunch', 'I want to eat a tuna sandwich for lunch']\n",
      "COSINE SIMILARITY: 0.98094356\n",
      "*****************************************************************\n"
     ]
    }
   ],
   "source": [
    "# PART V: insert code in this cell and below. Insert new cells as needed\n",
    "for i in range(len(sentence_pairs)):\n",
    "    print(f\"({i+1}) {sentence_pairs[i]}\")\n",
    "    da_embed = calc_doc_embedding(sentence_pairs[i][0], model, 100, dict())\n",
    "    db_embed = calc_doc_embedding(sentence_pairs[i][1], model, 100, dict())\n",
    "    print(\"COSINE SIMILARITY:\",cosine_similarity(da_embed, db_embed)[0][0])\n",
    "    print(\"*****************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI: Similarity in Corpus - 30 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Compute the cosine similarity of corpus[15] and all other documents and display the top 10 documents with the highest similarity. You must display BOTH the documents (reviews) and their respective scores. Corpus[15] should be the following review:\n",
    "\n",
    "<i>the s9000 fits perfectly in the case . too bad the case has no strap and no belt loop . apparently , the camera strap is supposed to function as the case strap also . too bad the camera strap is extremely short , made for very small people . so , instead of returning it and using a generic case that would not protectthe camera as well , i will adapt the case to work for me . i will get a bigger aftermarket strap for the camera . i can take the case to my seamstress and have a belt loop sewn on . it will work but only after extensive modification . great camera design so it 's odd that the case appears to have been designed by someone who has never used a camera before</i>\n",
    "\n",
    "(2) Plot a bar chart showing the distribution of the similarities for ALL documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART VI (1) put your code after this line and \n",
    "# run the cell to display the top 10 reviews and their similarities\n",
    "# you can use multiple cells\n",
    "def get_similarities_and_report(idx, corpus):\n",
    "    \"\"\"\n",
    "    idx: index of document to compare all other documents to\n",
    "    corpus: list of reviews\n",
    "    This function first computes the doc_embedding for the doc of interest and then iterates through the corpus, \n",
    "    skipping the document at index idx and calculating all other doc embeddings and cosine similarities\n",
    "    The top 10 report and histogram are generated\n",
    "    \"\"\"\n",
    "    review = corpus[idx]\n",
    "    review_embed = calc_doc_embedding(review, model, 100, dict())\n",
    "    list_of_similarities = list()\n",
    "    for i in range(len(corpus)):\n",
    "        if i != idx:\n",
    "            di_embed = calc_doc_embedding(corpus[i], model, 100, dict())\n",
    "            cos_sim = cosine_similarity(review_embed, di_embed)[0][0]\n",
    "            list_of_similarities.append(cos_sim)\n",
    "    mydata = pd.DataFrame({'Cosine': cos})\n",
    "    mydata = mydata.sort_values(by='Cosine', ascending=False)\n",
    "    mydata.head(10)\n",
    "    for i in range(10):\n",
    "        print('Cosine similarity:', mydata['Cosine'].iloc[i], '\\n')\n",
    "        print(corpus[mydata.index[i]], '\\n')\n",
    "    \n",
    "    plt.hist(get_list_of_similarities(15, corpus),bins=40);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9686222498937276 \n",
      "\n",
      "for average people who want a camera that just works-and takes good pics you should not buy this . i do n't know about the specifics , but like other reviewers mine got stuck and quit working for no reason after a couple of months . so , i do n't really see what all the bells and whistles are for if it does n't even take a picture . it was fine while it worked , it ate batteries but the pics were good \n",
      "\n",
      "Cosine similarity: 0.963819487703435 \n",
      "\n",
      "it 's not a conflict of good and evil that drives millions ( fox ) , but rather issues of what 's right and wrong . set in the uk , this exquisite family film about two brothers , serious seven year old damien and fun-loving nine year anthony , who recently lost their mom and discover a suitcase full of british pounds has dropped out of the sky and crashed into their playhouse . and it 's only seven days before the official currency switches to euros . the brothers think it came from god and they have differing agendas : spend it on themselves or give it to the needy . but the money , on a train to incinerators , was tossed by crooks to accomplices and they want the missing bag back . millions is funny , scary and profound . you may even discover things about yourself you never before considered . rated : pg . genre : family drama / comedy . 1 hour , 38 minutes . starring : james nesbitt , daisy donovan , alex etel , lewis mcgibbon . director : danny boyle . \n",
      "\n",
      "Cosine similarity: 0.9618662033951562 \n",
      "\n",
      "the writing was not what i expect of andre norton . too many gaps and information that is disjointed . whether this is a consequence of collaboration i do n't know . i felt this should have had a strong editor to point out where things should be tightened . for example - at the start we know kirion has sent people to capture his sister . the book covers over 2 years , but you hardly hear of this again . i cannot imagine the sorcerer would just have let it drop . i am a long-time fan and retired children 's librarian . sic fi & fantasy is my favorite genre , but this one i had to push to finish . unsatisfying . \n",
      "\n",
      "Cosine similarity: 0.9606910028544204 \n",
      "\n",
      "this is one of my favorite releases from common . this chi town warrior crafted an underground release that will bring a vibrant smile to the hardcore hip hop head and even awaken the minds of some of the fake commercial rap artist lovers . songs like i used to love h.e.r and ressurection start the classic album and take you on the road to classic chicago hip hop . not only the lyrics are dope but also the beats and the hooks . this is a pivotal album that needs to be in everyone 's collectio \n",
      "\n",
      "Cosine similarity: 0.9592990895419864 \n",
      "\n",
      "as a dad , i was really looking forward to a book on potty training to help me with my 2-year old . this book is lousy though . why ? 1. stupid writing . i got pretty sick of constantly reading all the sappy , \" witty \" remarks . \" dancing the potty mambo \" ? give me a break ! 2. stupid advice . this book keeps telling you to talk to your kid like he / she is an adult . i 'm sorry , but a 2-year-old just cannot comprehend a long drawn-out explanation of why you want them or do n't want them to do something . how about just saying , \" do n't do that ! \" or \" hey , you want to try this ? \" that seems to work just fine for me . why give your kid a doctoral dissertation every time you want them to do something ? what 's wrong with just telling your kid what to do ( nicely ) , and expecting them to be obedient ? 3. repeating content . this book kept repeating itself over and over . okay , i got it already ! i strongly feel that the author wanted to push this book over the 200-page mark when all that needed to be said could probably have been accomplished in about 50 pages ( or less ) . 4. too many references . this book constantly refers to other chapters . i once counted 6 different references to other chapters on one page . i am not kidding ! huh ? is potty training really that freakin ' complicated ? all i want is some general ideas on how to go about it , how to get started , and a few good tips . i do n't need all these detailed references . ( nor do i need the phrase \" potty mambo \" repeated to me ad infinitum . ) here is all i wanted to know : -how do i get started ? -what do i do ? -how often do i need to try it ? -should i go for pullups or just keep him in diapers for the moment ? maybe this book got to all of that , but i quit after page 60. i just could n't take it anymore . after hearing the phrases \" stay positive \" and \" potty mambo \" repeated to me for the umpteenth time , i had to quit . i returned this book and got my money back . if i were to say anything good about this book , i would say that it has a few good tips , surrounded by about 180 pages of filler . potty training still looms ahead . will i be successful ? probably . but to this book i say , no thanks \n",
      "\n",
      "Cosine similarity: 0.9591083363690863 \n",
      "\n",
      "this seemingly simple piece of molded plastic was recommended to me by a professional photographer . basically it 's the shape of an open box that slides firmly onto the flash . it is small and sturdy enough to stash in your camera bag without worrying about damage . when used the light disperses more evenly and is less harsh on the subject being photographed . the white box is for general use . also available are the green omni-bounce for florescent lighting and the gold omni-bounce for a warming effect . pros : inexpensive solution for better flash photography . a quality product that works . cons : non \n",
      "\n",
      "Cosine similarity: 0.958878169080006 \n",
      "\n",
      "this is as bad as the film.in fact just like the film the soundtrack is the same old formula.will&amp ; his movies&amp ; music never change the same old beat.the rest of the soundtrack is just as bad.unoriginal&amp ; boring.what a waste of kool moe dee \n",
      "\n",
      "Cosine similarity: 0.9587728180927098 \n",
      "\n",
      "tiger is simply the most advanced personal operational system on earth ! i am an advanced used . i have used all sorts of operational systems in the last decades : irix ( unix from silicon graphics ) , sunos ( unix from sun ) , linux , and also the bad written windows . everytime i used windoze in the last 20 years , i had to deal with crashes , hanging , lost of data and hours of configurations , tuning , adjusts , cleaning , etc. even today , everytime i shutdown windoze , it hangs , due to some program crashing . it 's a true joke ! ! ! on february 2005 i decided to buy a mac and give it a try . i bought a mac mini and started using it . what i have to say is that i am using it for more than an year without an issue , without a crash . system is solid as rock . some bad written programs eventually crash , but never the system . and i am the kind of guy who works with several heavy programs with heavy documents opened at the same time . mac never crashed . i am about to suggest apple a way to make it crash , so we can rest a little bit , instead of working all the time . so much trouble i had in the past with windoze that even today , while i am using the mac , i feel like it will crash at any time . fortunately it never crashed . beyond those functions advertised by apple , you will see that macos is a superb gold mine for those who want to explore under the hood . filled with thousands of powerfull unix commands and programs , you will be able to perform miracles . microsoft is copying everything it can from tiger and advertising it as revolutionary features of up comming windows ( hasta la ) vista . i cannot see a single revolutionary function on vista , as everything already exists on tiger . the most amazing is some sites on the web comparing vista ( a vaporware not yet launched ) with tiger ( an actual product ) . the only thing gates forgot is that one month after vista is released , apple will launch leopard that will take the crown from tiger . what i can say you is that for the first time on years i have joy of using a personal computer . \n",
      "\n",
      "Cosine similarity: 0.9587527106891643 \n",
      "\n",
      "while the title cut with joe public was bumping most of this disc is so-so&amp ; formula driven.without teddy riley the grooves&amp ; vibe are n't as strong \n",
      "\n",
      "Cosine similarity: 0.9586037559492239 \n",
      "\n",
      "imagine buying this cd after seeing \" the mirror has two faces \" fully expecting the soundtrack music to the movie , and reading overall great reviews on amazon . imagine the shock of receiving the cd and the surprise of hearing 20 brief cuts of plain instrumental music out of the 24 cuts - the remaining 4 cuts contained very short lines of verse . perhaps barbara , with her tremendous unlimited wealth , felt she would be giving away a \" free album \" to the masses if the soundtrack contained the vocals in the film . shameless misrepresentation \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(2) Plot the bar chart showing the similarities for all the documents in the corpus\n",
    "# you may use the following code template:\n",
    "get_similarities_and_report(15, corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VII - 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat Part VI  for another review of your choice in the corpus and show your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9686222498937276 \n",
      "\n",
      "for average people who want a camera that just works-and takes good pics you should not buy this . i do n't know about the specifics , but like other reviewers mine got stuck and quit working for no reason after a couple of months . so , i do n't really see what all the bells and whistles are for if it does n't even take a picture . it was fine while it worked , it ate batteries but the pics were good \n",
      "\n",
      "Cosine similarity: 0.963819487703435 \n",
      "\n",
      "it 's not a conflict of good and evil that drives millions ( fox ) , but rather issues of what 's right and wrong . set in the uk , this exquisite family film about two brothers , serious seven year old damien and fun-loving nine year anthony , who recently lost their mom and discover a suitcase full of british pounds has dropped out of the sky and crashed into their playhouse . and it 's only seven days before the official currency switches to euros . the brothers think it came from god and they have differing agendas : spend it on themselves or give it to the needy . but the money , on a train to incinerators , was tossed by crooks to accomplices and they want the missing bag back . millions is funny , scary and profound . you may even discover things about yourself you never before considered . rated : pg . genre : family drama / comedy . 1 hour , 38 minutes . starring : james nesbitt , daisy donovan , alex etel , lewis mcgibbon . director : danny boyle . \n",
      "\n",
      "Cosine similarity: 0.9618662033951562 \n",
      "\n",
      "the writing was not what i expect of andre norton . too many gaps and information that is disjointed . whether this is a consequence of collaboration i do n't know . i felt this should have had a strong editor to point out where things should be tightened . for example - at the start we know kirion has sent people to capture his sister . the book covers over 2 years , but you hardly hear of this again . i cannot imagine the sorcerer would just have let it drop . i am a long-time fan and retired children 's librarian . sic fi & fantasy is my favorite genre , but this one i had to push to finish . unsatisfying . \n",
      "\n",
      "Cosine similarity: 0.9606910028544204 \n",
      "\n",
      "this is one of my favorite releases from common . this chi town warrior crafted an underground release that will bring a vibrant smile to the hardcore hip hop head and even awaken the minds of some of the fake commercial rap artist lovers . songs like i used to love h.e.r and ressurection start the classic album and take you on the road to classic chicago hip hop . not only the lyrics are dope but also the beats and the hooks . this is a pivotal album that needs to be in everyone 's collectio \n",
      "\n",
      "Cosine similarity: 0.9592990895419864 \n",
      "\n",
      "as a dad , i was really looking forward to a book on potty training to help me with my 2-year old . this book is lousy though . why ? 1. stupid writing . i got pretty sick of constantly reading all the sappy , \" witty \" remarks . \" dancing the potty mambo \" ? give me a break ! 2. stupid advice . this book keeps telling you to talk to your kid like he / she is an adult . i 'm sorry , but a 2-year-old just cannot comprehend a long drawn-out explanation of why you want them or do n't want them to do something . how about just saying , \" do n't do that ! \" or \" hey , you want to try this ? \" that seems to work just fine for me . why give your kid a doctoral dissertation every time you want them to do something ? what 's wrong with just telling your kid what to do ( nicely ) , and expecting them to be obedient ? 3. repeating content . this book kept repeating itself over and over . okay , i got it already ! i strongly feel that the author wanted to push this book over the 200-page mark when all that needed to be said could probably have been accomplished in about 50 pages ( or less ) . 4. too many references . this book constantly refers to other chapters . i once counted 6 different references to other chapters on one page . i am not kidding ! huh ? is potty training really that freakin ' complicated ? all i want is some general ideas on how to go about it , how to get started , and a few good tips . i do n't need all these detailed references . ( nor do i need the phrase \" potty mambo \" repeated to me ad infinitum . ) here is all i wanted to know : -how do i get started ? -what do i do ? -how often do i need to try it ? -should i go for pullups or just keep him in diapers for the moment ? maybe this book got to all of that , but i quit after page 60. i just could n't take it anymore . after hearing the phrases \" stay positive \" and \" potty mambo \" repeated to me for the umpteenth time , i had to quit . i returned this book and got my money back . if i were to say anything good about this book , i would say that it has a few good tips , surrounded by about 180 pages of filler . potty training still looms ahead . will i be successful ? probably . but to this book i say , no thanks \n",
      "\n",
      "Cosine similarity: 0.9591083363690863 \n",
      "\n",
      "this seemingly simple piece of molded plastic was recommended to me by a professional photographer . basically it 's the shape of an open box that slides firmly onto the flash . it is small and sturdy enough to stash in your camera bag without worrying about damage . when used the light disperses more evenly and is less harsh on the subject being photographed . the white box is for general use . also available are the green omni-bounce for florescent lighting and the gold omni-bounce for a warming effect . pros : inexpensive solution for better flash photography . a quality product that works . cons : non \n",
      "\n",
      "Cosine similarity: 0.958878169080006 \n",
      "\n",
      "this is as bad as the film.in fact just like the film the soundtrack is the same old formula.will&amp ; his movies&amp ; music never change the same old beat.the rest of the soundtrack is just as bad.unoriginal&amp ; boring.what a waste of kool moe dee \n",
      "\n",
      "Cosine similarity: 0.9587728180927098 \n",
      "\n",
      "tiger is simply the most advanced personal operational system on earth ! i am an advanced used . i have used all sorts of operational systems in the last decades : irix ( unix from silicon graphics ) , sunos ( unix from sun ) , linux , and also the bad written windows . everytime i used windoze in the last 20 years , i had to deal with crashes , hanging , lost of data and hours of configurations , tuning , adjusts , cleaning , etc. even today , everytime i shutdown windoze , it hangs , due to some program crashing . it 's a true joke ! ! ! on february 2005 i decided to buy a mac and give it a try . i bought a mac mini and started using it . what i have to say is that i am using it for more than an year without an issue , without a crash . system is solid as rock . some bad written programs eventually crash , but never the system . and i am the kind of guy who works with several heavy programs with heavy documents opened at the same time . mac never crashed . i am about to suggest apple a way to make it crash , so we can rest a little bit , instead of working all the time . so much trouble i had in the past with windoze that even today , while i am using the mac , i feel like it will crash at any time . fortunately it never crashed . beyond those functions advertised by apple , you will see that macos is a superb gold mine for those who want to explore under the hood . filled with thousands of powerfull unix commands and programs , you will be able to perform miracles . microsoft is copying everything it can from tiger and advertising it as revolutionary features of up comming windows ( hasta la ) vista . i cannot see a single revolutionary function on vista , as everything already exists on tiger . the most amazing is some sites on the web comparing vista ( a vaporware not yet launched ) with tiger ( an actual product ) . the only thing gates forgot is that one month after vista is released , apple will launch leopard that will take the crown from tiger . what i can say you is that for the first time on years i have joy of using a personal computer . \n",
      "\n",
      "Cosine similarity: 0.9587527106891643 \n",
      "\n",
      "while the title cut with joe public was bumping most of this disc is so-so&amp ; formula driven.without teddy riley the grooves&amp ; vibe are n't as strong \n",
      "\n",
      "Cosine similarity: 0.9586037559492239 \n",
      "\n",
      "imagine buying this cd after seeing \" the mirror has two faces \" fully expecting the soundtrack music to the movie , and reading overall great reviews on amazon . imagine the shock of receiving the cd and the surprise of hearing 20 brief cuts of plain instrumental music out of the 24 cuts - the remaining 4 cuts contained very short lines of verse . perhaps barbara , with her tremendous unlimited wealth , felt she would be giving away a \" free album \" to the masses if the soundtrack contained the vocals in the film . shameless misrepresentation \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_similarities_and_report(64, corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VIII Observations - 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write 5-10 lines describing your observations in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part III:** <br> \n",
    "Two words can be similar even if they are opposites. We can take for example Happy and Sad\n",
    "<br>\n",
    "**-Part V:** <br>\n",
    "When we look at the examples, we see that overall, the document pairs are very similar. However,because Tuna and Sandwich occur often together, the cosine similarity for document pair 4 is higher than that of document 1 because the latter has more words.\n",
    "<br>\n",
    "**Part VI and VII:**<br>\n",
    "We notice that the histogram  shows that most documents have similarities between 0.8 and 1.0\n",
    "Very few negative similarities are observed.\n",
    "One potential limiting factor is the embedding size which is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.051px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
