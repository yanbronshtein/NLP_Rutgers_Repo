{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment requires you to compute the TF-IDF measures for words in a collection of documents or corpus. TF-IDF stands for Term Frequency-Inverse Document Frequency and measures the relevance of a term <i><b>t</b></i> (t can be a word or a sequence of words called an n-gram) for a document in a collection. It is a combination of two measures as follows:\n",
    "\n",
    "1. Term Frequency (TF) measures the frequency of the term t in a document d. If the term occurs n times in a document, then TF is usually the normalized frequency and is defined as:\n",
    "\n",
    "\tTF(t, d) = n/(number of terms in d)\n",
    "\n",
    "If the TF measure is high, then the term occurs frequently in the document and is considered relevant for that document. \n",
    "\n",
    "2. Inverse Document Frequency (IDF) is\n",
    "\n",
    "\tIDF(t) = log(total number of documents/number of documents containing t)\n",
    "\n",
    "IDF measures how important a term is in the collection. Terms such as \"and\" and \"the\" may occur across all documents and are not considered relevant. So, the IDF value for such terms will be low. \n",
    "\n",
    "Finally, TF-IDF(t, d) = TF(t,d) * IDF(t)\n",
    "\n",
    "For an example, see the example section at http://www.tfidf.com  . \n",
    "\n",
    "Your corpus could be a single file where each line or paragraph is a document or it could be a directory \n",
    "with multiple files where each file is a document. See the attached hw1-input.txt for a trivial example of two documents. Each line is a document here and a newline character is the indication for the end of the document. We will use this example to illustrate the computation of TF-IDF. \n",
    "\n",
    "You may choose to represent the document collection as a Python list of strings in your program. See the code cell below for a representation of the two documents in hw1-input.txt. We will refer to this document collection as the <i><b>sample document collection</b></i> in this homework. \n",
    "\n",
    "IMPORTANT:\n",
    "\n",
    "(1) You need to strip out ALL punctuation and convert words to lowercase in your program.\n",
    "\n",
    "(2)<b> You cannot use any Python library that computes the TF-IDF scores. </b>\n",
    "    \n",
    "(3) <b>  You are required to use NumPy arrays and the functions/techniques discussed in class to find the denominators in the TF and IDF measures in Parts III and IV </b> (number of terms in document and number of documents containing t). If you use naive loops to count these measures, you will lose points. Both these measures require you to compute something across the rows/columns. Think of which aggregate functions you could you use in NumPy to do this easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document_collection = ['The car is driven on the road', 'The truck is driven on the highway']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The car is driven on the road', 'The truck is driven on the highway']\n"
     ]
    }
   ],
   "source": [
    "print(sample_document_collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I  - 10 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called doc_vocab that takes a document corpus (could be a filename or a directory name with multiple files in it. You can assume the former for simplicity.) and returns a (1) vocabulary of words for the corpus and (2) the document collection (a list of documents). \n",
    "\n",
    "You can implement the vocabulary as a dictionary where the key is the word and the value (to which the key is mapped) is a unique index assigned to the word. \n",
    "\n",
    "The dictionary for the sample document collection could be something like \n",
    "{'the': 0, 'car': 1, 'is': 2, 'driven': 3, 'on': 4, 'road': 5, 'truck': 6, 'highway': 7}\n",
    "\n",
    "Note that the actual indices you have could be different. They just have to be unique and consecutive i.e. for n words in the dictionary, the indices should range from 0 to n-1. Also note that words \"the\" and \"The\" are assumed to be the same.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART I: build lexicon and document collection\n",
    "def doc_vocab(filename):\n",
    "    ## fill in your code here\n",
    "    # you may import libraries and define additional helper functions outside this function\n",
    "    # lexicon is a synonym for vocabulary\n",
    "    \n",
    "    return lexicon, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - 20 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called <i><b>doc_term_matrix</i></b> that takes a vocabulary and a document collection as parameters and returns a document term matrix. This matrix should be a two-dimensional NumPy array with the following shape: (number of documents in collection, number of terms in vocabulary). For the sample document collection, the array's shape will be (2, 8). If the array is M, then M[i, j] will contain the number of times term j occurs in document i. For the sample document collection, you may get something like \n",
    "\n",
    "[[2 1 1 1 1 1 0 0]\n",
    "\n",
    " [2 0 1 1 1 0 1 1]] . \n",
    "\n",
    "Note that the columns may be arranged differently based on the indices for the terms in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 2\n",
      "key: 1\n",
      "is: 1\n",
      "in: 1\n",
      "car: 1\n"
     ]
    }
   ],
   "source": [
    "# You will need to initialize a NumPy array to store the document term matrix. \n",
    "# Which NumPy function will you use for this? What are the dimensions of this matrix?\n",
    "# Fill this NumPy array with the appropriate values and return it as the value for this function\n",
    "#You may use the collections package to count the number of times an element occurs in a list. \n",
    "#Note a use of collections below. You can modify it for use in Part II\n",
    "import collections\n",
    "dictfreq = collections.Counter(\"the key is in the car\".split())\n",
    "for key in dictfreq.keys():\n",
    "    print(key+\": \"+str(dictfreq[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART II: document term matrix\n",
    "\n",
    "def doc_term_matrix(vocabulary, collection):\n",
    "    # fill in your code here\n",
    "    \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III - 20 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called <i><b>tf_matrix</i></b> that takes a document term matrix and returns a normalized frequency matrix that represents the TF scores for each term in each document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART III: normalized document term matrix\n",
    "# Again, initialize a NumPy array with the appropriate dimensions and \n",
    "# then fill it with the appropriate values.\n",
    "# You must use a NumPy aggregation function for computing the TF scores\n",
    "# Think about making your code succinct.\n",
    "\n",
    "def tf_matrix(document_term_matrix):\n",
    "    # fill in your code here\n",
    "    \n",
    "    return norm_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV - 20 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called <i><b>tf_idf_matrix</i></b> that takes a document term matrix and a normalized frequency matrix and returns a matrix that represents the TF-IDF scores for each term in the document. You can use np.log10 to compute the logarithm here. For the sample document collection, the function could return something like \n",
    "\n",
    "[[0.0 &nbsp;  0.04300429 &nbsp; 0.0 &nbsp; 0.0  &nbsp;  0.0 &nbsp;  0.04300429 &nbsp;   0.0 &nbsp;  0.0 &nbsp;]\n",
    "  \n",
    " [0.0 &nbsp; 0.0 &nbsp;  0.0 &nbsp;  0.0 &nbsp;  0.0 &nbsp;  0.0 &nbsp; 0.04300429 &nbsp; 0.04300429 &nbsp;]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART IV: TF_IDF matrix\n",
    "\n",
    "def tf_idf_matrix(document_term_matrix, tf_matrix):\n",
    "    # fill in your code here\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to test your functions for the trivial test case: hw1-input.txt. \n",
    "You must print out \n",
    "\n",
    "(1) the vocabulary\n",
    "\n",
    "(2) the document frequency matrix\n",
    "\n",
    "(3) the normalized frequency matrix\n",
    "\n",
    "(4) the tf-idf matrix \n",
    "\n",
    "\n",
    "Label each output with what it is. E.g. VOCABULARY, DOCUMENT FREQUENCY MATRIX, and so on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI - 20 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your code with a more complex input file(s). You may use <b><i>beatles_biography.txt</b></i> taken from https://www.notablebiographies.com/Ba-Be/Beatles.html. In this case, the documents could be defined by the newlines in the file. You are free to collapse the paragraphs to create fewer documents. You may also use other documents about the Beatles to do a broader analysis or you can choose a set of documents about a different topic. This part of the assignment is open-ended. \n",
    "\n",
    "Using the aggregate functions in NumPy, print out the maximum tf-idf scores for each document and their corresponding words. Note that it may be interesting to pick a few top words (words that have tf-idf scores close to the maximum). \n",
    "\n",
    "You can choose to (1) have a list of stop words to filter the documents and (2) have a list of words (say, 1-3 words i.e. n-grams) for each term instead of a single word. You may have to modify your function definitions in this case. Show your modified functions in this section. This part is optional and for extra credit only.\n",
    "\n",
    "Explain your methods clearly and write your observations from your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Your submission must be an Jupyter Notebook (.ipynb) file that is organized exactly like this notebook file. You can insert cells (or use the cells provided) after the instructions for each section in this file and then insert your code into the cells. To write your observations for Part VI, change the type of the cell to Markdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Markdown cell unlike the code cell above."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
