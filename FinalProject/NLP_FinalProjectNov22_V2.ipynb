{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yanivbronshtein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import nltk.corpus\n",
    "\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import SimpleRNN\n",
    "# from keras.layers import LSTM\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "stopset = stopwords.words('english') + list(string.punctuation)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df = df.groupby(by='category').head(1000)\n",
    "df['summary'] = df['headline'] + '. ' +  df['short_description']\n",
    "df = df[['summary','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = ['POLITICS', 'ENTERTAINMENT', 'TRAVEL', 'BUSINESS', 'SPORTS', 'RELIGION']\n",
    "df_news = df[df['category'].isin(category_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary       category\n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT\n",
       "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT\n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  ENTERTAINMENT\n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT\n",
       "5  Morgan Freeman 'Devastated' That Sexual Harass...  ENTERTAINMENT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try the pd.get_dummies() approach instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news2 = pd.get_dummies(df_news, columns=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>category_BUSINESS</th>\n",
       "      <th>category_ENTERTAINMENT</th>\n",
       "      <th>category_POLITICS</th>\n",
       "      <th>category_RELIGION</th>\n",
       "      <th>category_SPORTS</th>\n",
       "      <th>category_TRAVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  category_BUSINESS  \\\n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...                  0   \n",
       "\n",
       "   category_ENTERTAINMENT  category_POLITICS  category_RELIGION  \\\n",
       "1                       1                  0                  0   \n",
       "\n",
       "   category_SPORTS  category_TRAVEL  \n",
       "1                0                0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df_news2['summary']\n",
    "all_labels = df_news2.drop(columns=['summary'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_labels, train_size=.8, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check matching dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in the training data and training labels match.\n"
     ]
    }
   ],
   "source": [
    "assert(len(X_train) == len(y_train))\n",
    "print('The number of samples in the training data and training labels match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in the test data and test labels match.\n"
     ]
    }
   ],
   "source": [
    "assert(len(X_test) == len(y_test))\n",
    "print('The number of samples in the test data and test labels match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() #make Tfidf Vectorizer\n",
    "tfidf_encodings = vectorizer.fit_transform(X_train) #encode the text\n",
    "                   \n",
    "# df['tfidf'] = list(tfidf_encodings.toarray()) #vectorized texts to dense list format for storage in dataframe\n",
    "\n",
    "# vectors_for_training = np.array(df['tfidf'].tolist()) #get the vectors back out of the dataframe for use in something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node sequential_2/Cast\n (defined at /Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py:671)\n]] [Op:__inference_train_function_3358]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_2/Cast:\nIn[0] IteratorGetNext (defined at /Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     # Remove the mapping before closing the asyncio loop. If this\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n>>>     except asyncio.CancelledError:\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n>>>     The name of the module to be executed.\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n>>>     silent : bool\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n>>>     self.events.trigger('pre_run_cell', info)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     \"\"\"Run a sequence of AST nodes. The execution mode depends on the\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n>>>     # If interactivity is async the semantics of run_code are\n>>> \n>>>   File \"<ipython-input-74-8324f2be2095>\", line 15, in <module>\n>>>     validation_data=(X_test, y_test))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 373, in call\n>>>     return super(Sequential, self).call(inputs, training=training, mask=mask)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 571, in _run_internal_graph\n>>>     y = self._conform_to_reference_input(y, ref_input=x)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 671, in _conform_to_reference_input\n>>>     tensor = tf.cast(tensor, dtype=ref_input.dtype)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-8324f2be2095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Error: Cast string to float is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node sequential_2/Cast\n (defined at /Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py:671)\n]] [Op:__inference_train_function_3358]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_2/Cast:\nIn[0] IteratorGetNext (defined at /Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     # Remove the mapping before closing the asyncio loop. If this\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/base_events.py\", line 538, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1782, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n>>>     except asyncio.CancelledError:\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n>>>     user_expressions, allow_stdin,\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n>>>     The name of the module to be executed.\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n>>>     silent : bool\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n>>>     self.events.trigger('pre_run_cell', info)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n>>>     \"\"\"Run a sequence of AST nodes. The execution mode depends on the\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n>>>     # If interactivity is async the semantics of run_code are\n>>> \n>>>   File \"<ipython-input-74-8324f2be2095>\", line 15, in <module>\n>>>     validation_data=(X_test, y_test))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\", line 373, in call\n>>>     return super(Sequential, self).call(inputs, training=training, mask=mask)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 571, in _run_internal_graph\n>>>     y = self._conform_to_reference_input(y, ref_input=x)\n>>> \n>>>   File \"/Users/yanivbronshtein/opt/anaconda3/lib/python3.7/site-packages/keras/engine/functional.py\", line 671, in _conform_to_reference_input\n>>>     tensor = tf.cast(tensor, dtype=ref_input.dtype)\n>>> "
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(tfidf_encodings, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Error: Cast string to float is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    Will Smith Joins Diplo And Nicky Jam For The 2...\n",
      "2    Hugh Grant Marries For The First Time At Age 5...\n",
      "Name: summary, dtype: object \n",
      "\n",
      "1    ENTERTAINMENT\n",
      "2    ENTERTAINMENT\n",
      "Name: category, dtype: object \n",
      "\n",
      "Total number of reviews: 6000\n",
      "Total number of labels: 6000\n"
     ]
    }
   ],
   "source": [
    "all_data = df_news['summary']\n",
    "all_labels = df_news['category']\n",
    "\n",
    "print(all_data[:2], '\\n')\n",
    "print(all_labels[:2], '\\n')\n",
    "\n",
    "print('Total number of reviews:', len(all_data))\n",
    "print('Total number of labels:', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X_train) == len(y_train))\n",
    "print('The number of samples in the training data and training labels match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X_test) == len(y_test))\n",
    "print('The number of samples in the test data and test labels match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(X_train, y_train); labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('True Labeling'); plt.ylabel('Predicted Labeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MLPClassifier(verbose=True))\n",
    "model.fit(X_train, y_train); labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('true label'); plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MLPClassifier(hidden_layer_sizes=(100,100),verbose=True))\n",
    "model.fit(X_train, y_train); labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('true label'); plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MLPClassifier(hidden_layer_sizes=(100,100,100),verbose=True))\n",
    "model.fit(X_train, y_train); labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('true label'); plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('true label'); plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('C:/Users/benja/Downloads/glove.6B.100d.txt')\n",
    "word2vec_text_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(word2vec_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_embedding(doc, embeddings, embed_size, OOV_embed):\n",
    "    word_count = 0\n",
    "    doc_embed = np.zeros(embed_size, dtype=float)\n",
    "    \n",
    "    for word in doc.split():\n",
    "        if word not in stopset:\n",
    "            if (word in embeddings):\n",
    "                word_embedding = embeddings[word]\n",
    "                \n",
    "            elif word in OOV_embed:\n",
    "                word_embedding = OOV_embed[word]\n",
    "                \n",
    "            else:\n",
    "                word_embedding = np.random.random(embed_size)\n",
    "                OOV_embed[word] = word_embedding\n",
    "                \n",
    "            doc_embed += word_embedding\n",
    "            word_count += 1\n",
    "            \n",
    "    doc_embed /= word_count\n",
    "    return doc_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_embed = {}\n",
    "X_train_embed = np.vstack([calc_doc_embedding(doc, wv, 100, OOV_embed) for doc in X_train])\n",
    "X_test_embed = np.vstack([calc_doc_embedding(doc, wv, 100, OOV_embed) for doc in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(verbose=True)\n",
    "model.fit(X_train_embed, y_train); labels = model.predict(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('True Labeling'); plt.ylabel('Predicted Labeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(100,100), verbose=True)\n",
    "model.fit(X_train_embed, y_train); labels = model.predict(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('True Labeling'); plt.ylabel('Predicted Labeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(100,100,100), verbose=True)\n",
    "model.fit(X_train_embed, y_train); labels = model.predict(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, labels)\n",
    "plt.figure(figsize=(2,2))\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "\n",
    "plt.xlabel('True Labeling'); plt.ylabel('Predicted Labeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Microaveraged F1 score:', f1_score(y_test, labels, average='micro'))\n",
    "print('Macroaveraged F1 score:', f1_score(y_test, labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('C:/Users/benja/Downloads/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000; maxlen=100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train_embed, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_embed, y_test))\n",
    "\n",
    "# Error: Cast string to float is not supported"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
